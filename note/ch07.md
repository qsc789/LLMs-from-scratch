# 第 7 章：按照说明进行微调
![](/note/assets/img/ch07/ch07_01.png)
## 7.1 指令微调介绍
在第 5 章中，我们看到预训练 LLM 涉及一个训练过程，它学习一次生成一个单词,因此，预训练的 LLM 擅长文本补全，但不擅长遵循指令,在本章中，我们将教 LLM 更好地遵循指示
![](/note/assets/img/ch07/ch07_02.png)
![](/note/assets/img/ch07/ch07_04.png)
## 7.2 准备用于监督指令微调的数据集
+ 指令微调通常被称为“监督指令微调”，因为它涉及在明确提供输入-输出对的数据集上训练模型
+ 有多种方法可以将条目格式化为 LLM;下图说明了用于训练羊驼 （ https://crfm.stanford.edu/2023/03/13/alpaca.html） 和 Phi-3 （ https://arxiv.org/abs/2404.14219） LLMs
## 7.3 将数据组织到训练批次中
![](/note/assets/img/ch07/ch07_05.png)
我们分几个步骤处理此数据集批处理，如下图所示
![](/note/assets/img/ch07/ch07_06.png)
首先，我们实现一个 InstructionDataset 类，该类对数据集中的所有输入进行预标记，类似于第 6 章中的 SpamDataset
![](/note/assets/img/ch07/ch07_07.png)
在第 6 章中，我们将数据集中的所有示例填充到相同的长度
+ 在这里，我们采用更复杂的方法并开发一个自定义的 “collate” 函数，我们可以将其传递给数据加载器
+ 此自定义 collate 函数将每个批次中的训练样本填充为具有相同的长度（但不同的批次可以具有不同的长度
![](/note/assets/img/ch07/ch07_08.png)
![](/note/assets/img/ch07/ch07_09.png)
+ 在上面，我们只将输入返回到 LLM;但是，对于 LLM 训练，我们还需要 target 值
+ 与预训练 LLM，目标是向右移动 1 个位置的输入，因此 LLM 学习预测下一个标记
  ![](/note/assets/img/ch07/ch07_10.png)
在实践中，屏蔽与指令对应的目标 token ID 也很常见，如下图所示
  ![](/note/assets/img/ch07/ch07_11.png)
## 7.4 为指令数据集创建数据加载器
在本节中，我们使用 InstructionDataset 类和 custom_collate_fn 函数来实例化训练、验证和测试数据加载器
 ![](/note/assets/img/ch07/ch07_12.png)
 前面的 custom_collate_fn 函数的另一个额外细节是，我们现在直接将数据移动到目标设备（例如 GPU），而不是在主训练循环中执行此操作，这提高了效率，因为当我们使用 custom_collate_fn 作为数据加载器的一部分时，它可以作为后台进程执行
 使用 Python 的 functools 标准库中的 partial 函数，我们创建一个预先填充了原始函数的 device 参数的新函数
 ## 7.5 加载预训练LLM
 在本节中，我们使用在第 5 章第 5.5 节和第 6 章第 6.4 节中使用的相同代码加载预训练的 GPT 模型
  ![](/note/assets/img/ch07/ch07_13.png)
## 7.6 微调指令数据的 LLM
  ![](/note/assets/img/ch07/ch07_14.png)
  训练和验证损失曲线
  ![](/note/assets/img/ch07/ch07_15.png)
## 7.7 提取和保存响应
  ![](/note/assets/img/ch07/ch07_16.png)
  在本节中，我们将保存测试集响应，以便在下一节中进行评分,我们还会保存模型的副本以备将来使用
## 7.8 评估微调后的 LLM
![](/note/assets/img/ch07/ch07_17.png)
+ 在本节中，我们使用另一个更大的 LLM
+ 特别是，我们使用了 Meta AI 的指令微调的 80 亿参数 Llama 3 模型，该模型可以通过 ollama （https://ollama.com） 在本地运行
+ （或者，如果您更喜欢通过 OpenAI API 使用功能更强大的 LLM，如 GPT-4，请参阅 llm 笔记本）
+ Ollama 是一个高效运行 LLMs
+ 它是 llama.cpp （https://github.com/ggerganov/llama.cpp） 的包装器，在纯 C/C++ 中实现 LLMs 以最大限度地提高效率
+ 请注意，它是一个使用 LLMs 生成文本（推理）的工具，而不是训练或微调 LLMs
+ 在运行下面的代码之前，请访问 https://ollama.com 并按照说明安装 ollama（例如，单击 “下载” 按钮并下载适用于您的操作系统的 ollama 应用程序）


  