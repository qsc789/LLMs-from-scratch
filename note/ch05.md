# 第 5 章：未标记数据的预训练
## 5.1 评估生成文本模型
在本节开始时，我们简要回顾了如何使用上一章中的代码初始化 GPT 模型,然后，我们讨论了 LLMs,最后，在本节中，我们将这些评估指标应用于训练和验证数据集
### 5.1.1 使用 GPT 生成文本
我们使用上面的 0.1 dropout，但现在训练 LLMs 而不使用 dropout 是相对常见的,现代 LLMs 也不在 nn.查询矩阵、键矩阵和值矩阵的线性层（与早期的 GPT 模型不同），这是通过设置 “qkv_bias”来实现的：False,我们只减少了 256 个标记的上下文长度 （ context_length ），以减少训练模型的计算资源需求，而最初的 1.24 亿个参数的 GPT-2 模型使用了 1024 个标记,这样，更多的读者将能够在他们的笔记本电脑上遵循和执行代码示例,但是，请随时将context_length增加到 1024 个代币（这不需要任何代码更改）,稍后，我们还将从预训练的权重中加载一个 1024 context_length的模型,接下来，我们使用上一章的 generate_text_simple 函数来生成文本,此外，我们还定义了两个便捷函数 text_to_token_ids 和 token_ids_to_text，用于在本章中使用的标记和文本表示形式之间进行转换
![](/note/assets/img/ch05/ch05_01.png)
### 5.1.2 计算文本生成损失：交叉熵和困惑度
![](/note/assets/img/ch05/ch05_02.png)
![](/note/assets/img/ch05/ch05_03.png)
![](/note/assets/img/ch05/ch05_04.png)
![](/note/assets/img/ch05/ch05_05.png)
![](/note/assets/img/ch05/ch05_06.png)
![](/note/assets/img/ch05/ch05_07.png)
### 5.1.3 计算训练集和验证集损失
![](/note/assets/img/ch05/ch05_08.png)
![](/note/assets/img/ch05/ch05_09.png)
![](/note/assets/img/ch05/ch05_10.png)
## 5.2 训练一个 LLM
在本节中，我们最终实现用于训练 LLM 的代码,我们专注于一个简单的训练函数
![](/note/assets/img/ch05/ch05_11.png)
![](/note/assets/img/ch05/ch05_12.png)
查看上面的结果，我们可以看到该模型开始生成难以理解的单词字符串，而到最后，它能够生成语法上或多或少正确的句子,但是，根据训练和验证集损失，我们可以看到模型开始过拟合,请注意，这里发生过拟合是因为我们有一个非常非常小的训练集，并且我们迭代了很多次,我们无需花费数周或数月的时间在大量昂贵的硬件上训练此模型，而是稍后加载预训练的权重
![](/note/assets/img/ch05/ch05_13.png)
## 5.3 控制随机性的解码策略
### 5.3.1 温度缩放
![](/note/assets/img/ch05/ch05_14.png)
我们不是通过 torch.argmax 来确定最可能的令牌，而是通过 torch.multinomial(probas, num_samples=1) 从 softmax 分布中采样来确定最可能的令牌,为了便于说明，让我们看看当我们使用原始 softmax 概率对下一个代币采样 1,000 次时会发生什么.
我们可以通过一个叫做温度缩放的概念来控制分布和选择过程,“温度缩放”只是一个花哨的词，用于将 logits 除以大于 0 的数字,大于 1 的温度将导致在应用 softmax 后代币概率分布更均匀,小于 1 的温度将导致在应用 softmax 后出现更可靠（更尖锐或更尖锐）的分布
### 5.3.2 Top-k 采样
![](/note/assets/img/ch05/ch05_15.png)
### 5.3.3 修改文本生成功能
前两个小节介绍了温度采样和 top-k 采样,让我们使用这两个概念来修改我们之前通过 LLM，创建一个新的生成函数：
![](/note/assets/img/ch05/ch05_16.png)
## 5.4 在 PyTorch 中加载和保存模型权重
![](/note/assets/img/ch05/ch05_17.png)
## 5.5 从 OpenAI 加载预训练权重
以前，我们只使用一本非常小的短篇小说来训练一个小型的 GPT-2 模型，用于教育目的,有兴趣的读者也可以在 ../03_bonus_pretraining_on_gutenberg,幸运的是，我们不必花费数万到数十万美元在大型预训练语料库上对模型进行预训练，而是可以加载 OpenAI 提供的预训练权重,有关从 Hugging Face Hub 加载权重的替代方法，请参阅 ../02_alternative_weight_loading,首先，一些样板代码，用于从 OpenAI 下载文件并将权重加载到 Python 中,由于 OpenAI 使用了 TensorFlow，因此我们必须安装和使用 TensorFlow 来加载权重;tqdm 是一个进度条库,取消注释并运行下一个单元格以安装所需的库
![](/note/assets/img/ch05/ch05_18.png)