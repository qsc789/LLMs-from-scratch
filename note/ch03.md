# 编码注意力机制
## 3.1 长序列建模的问题
由于源语言和目标语言之间的语法结构存在差异，因此逐字翻译文本是不可行的
![](/note/assets/img/ch03/ch03_01.png)
在引入 transformer 模型之前，编码器-解码器 RNN 通常用于机器翻译任务
在此设置中，编码器使用隐藏状态（神经网络中的一种中间层）处理来自源语言的标记序列，以生成整个输入序列的压缩表示
![](/note/assets/img/ch03/ch03_02.png)
## 3.2 使用注意力机制捕获数据依赖关系
通过注意力机制，网络的文本生成解码器段能够选择性地访问所有输入标记，这意味着在生成特定输出标记时，某些输入标记比其他输入标记具有更大的意义：
![](/note/assets/img/ch03/ch03_03.png)
transformers 中的自我注意是一种技术，旨在通过使序列中的每个位置都能够参与并确定同一序列中所有其他位置的相关性来增强输入表示
![](/note/assets/img/ch03/ch03_04.png)
## 3.3 用自我注意关注输入的不同部分
### 3.3.1 没有可训练权重的简单自注意力机制
![](/note/assets/img/ch03/ch03_05.png)
![](/note/assets/img/ch03/ch03_06.png)
![](/note/assets/img/ch03/ch03_07.png)
![](/note/assets/img/ch03/ch03_08.png)
![](/note/assets/img/ch03/ch03_09.png)
### 3.3.2 计算所有输入标记的注意力权重
在上面，我们计算了输入 2 的注意力权重和上下文向量（如下图中突出显示的行所示）接下来，我们将此计算推广为计算所有注意力权重和上下文向量
![](/note/assets/img/ch03/ch03_10.png)
在自我注意中，该过程从计算注意力分数开始，随后对其进行标准化以得出总计为 1 的注意力权重,然后，这些注意力权重被用来通过输入的加权总和来生成上下文向量
![](/note/assets/img/ch03/ch03_11.png)
将前面的步骤 1 应用于所有成对元素，以计算非规范化的注意力分数矩阵：
## 3.4 使用可训练权重实现自我注意
![](/note/assets/img/ch03/ch03_12.png)
![](/note/assets/img/ch03/ch03_13.png)
![](/note/assets/img/ch03/ch03_14.png)
![](/note/assets/img/ch03/ch03_15.png)
我们使用之前使用的 softmax 函数计算注意力权重（总和为 1 的标准化注意力分数）
![](/note/assets/img/ch03/ch03_16.png)
### 3.4.2 实现一个紧凑的 SelfAttention 类
![](/note/assets/img/ch03/ch03_17.png)
我们可以使用 PyTorch 的线性层来简化上述实现，如果我们禁用偏置单元，则相当于矩阵乘法,使用 nn.线性超过我们的手动 nn.Parameter（torch.rand（...） 方法是 nn.Linear 具有首选的权重初始化方案，这导致更稳定的模型训练
## 3.5 用因果注意力隐藏将来的单词
### 3.5.1 应用因果注意力掩码
![](/note/assets/img/ch03/ch03_18.png)
掩盖未来注意力权重的最简单方法是通过 PyTorch 的 tril 函数创建一个掩码，其中主对角线下方的元素（包括对角线本身）设置为 1，主对角线上方的元素设置为 0：
![](/note/assets/img/ch03/ch03_19.png)
虽然我们现在在技术上已经完成了因果注意力机制的编码，但让我们简要地看一下一种更有效的方法来实现与上述相同的效果,因此，我们可以在对角线上方的未规范化注意力分数进入 softmax 函数之前用负无穷大掩盖它们，而不是将对角线上方的注意力权重归零并重新归一化结果
![](/note/assets/img/ch03/ch03_20.png)
### 3.5.2 使用 dropout 屏蔽额外的注意力权重
![](/note/assets/img/ch03/ch03_21.png)
![](/note/assets/img/ch03/ch03_22.png)
### 3.5.3 实现紧凑的因果自注意力类
![](/note/assets/img/ch03/ch03_23.png)
![](/note/assets/img/ch03/ch03_24.png)
## 3.6 将单头注意力扩展到多头注意力
### 3.6.1 堆叠多个单头注意力层
![](/note/assets/img/ch03/ch03_25.png)
![](/note/assets/img/ch03/ch03_26.png)
### 3.6.2 使用权重拆分实现多头注意力
虽然以上是多头注意力的直观且功能齐全的实现（包装了前面的单头注意力 CausalAttention 实现），但我们可以编写一个名为 MultiHeadAttention 的独立类来实现相同的效果,我们不会为这个独立的 MultiHeadAttention 类连接单个注意力头,相反，我们创建单个 W_query、W_key 和 W_value 权重矩阵，然后为每个注意力头将它们拆分为单独的矩阵
